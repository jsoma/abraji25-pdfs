{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet \"natural-pdf[ai,ocr-export] @ git+https://github.com/jsoma/natural-pdf.git\"\n",
    "!pip install --upgrade --quiet easyocr\n",
    "!pip install --upgrade --quiet surya-ocr\n",
    "# If you're on colab, Runtime > Restart Session, then move to the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607b3ac-3d41-454c-88e0-f096db6a3c67",
   "metadata": {},
   "source": [
    "# OCR: Recognizing text\n",
    "\n",
    "Sometimes you can't actually get the text off of the page. It's an *image* of text instead of being actual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f6224-1ec2-497e-be92-83cb65c8e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natural_pdf import PDF\n",
    "\n",
    "pdf = PDF(\"https://github.com/jsoma/abraji25-pdfs/raw/refs/heads/main/needs-ocr.pdf\")\n",
    "\n",
    "page = pdf.pages[0]\n",
    "page.show(width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd7508-24f5-4ba6-b8f6-bc82bc4c3e0e",
   "metadata": {},
   "source": [
    "Looks the same as the last one, right? But when we try to extract the text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72673727-f6cc-4db2-aba9-4da3145a83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cca14",
   "metadata": {},
   "source": [
    "Nothing! **It's time for OCR.**\n",
    "\n",
    "There are a looooot of OCR engines out there, and one of the things that makes Natural PDF nice is that it supports multiples. Figuring out which one is the \"best\" isn't as tough when you can just run them all right after each other.\n",
    "\n",
    "The default is [EasyOCR](https://github.com/JaidedAI/EasyOCR) which usually works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331e71a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page.apply_ocr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67180f0-e9dd-4ae7-9a0e-6b65a8261e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cd4de",
   "metadata": {},
   "source": [
    "I'm very iritated by the \"Durham's Pure Leaf Lardl\" instead of \"Durham's Pure Leaf Lard!\". Why'd it miss that??\n",
    "\n",
    "I don't need to know why, though, really, because I can just try some other engine! You can also fool around with the options - some of the the lowest-hanging fruit is increasing the resolution of the OCR. The default at the moment is 150, you can try upping to 300 for (potentially) better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting this out because for some reason surya is being very slow on colab?\n",
    "# page.apply_ocr('surya', resolution=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a54b89-f5bd-4b4e-adac-53192f702179",
   "metadata": {},
   "source": [
    "## Finding tables on OCR documents\n",
    "\n",
    "When we used `page.extract_table()` last time, it was easy because there were all of these `line` elements on the page that pdfplumber could detect and say \"hey, it's a table!\" For the same reason that there's no *real* text on the page, there's also no *real* lines on the page. Instead, we're going to do a fun secret trick where we look at what horizontal and vertical coordinates *seem* like they might be lines by setting a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100d453-4ce7-4c37-a519-88082737c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.extract_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3ecd1-4c47-44f1-bcb9-68d7965c9456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_area = (\n",
    "    page\n",
    "    .find('text:contains(Violations)')\n",
    "    .below(\n",
    "        until='text:contains(Jungle)',\n",
    "        include_endpoint=False\n",
    "    )\n",
    ")\n",
    "table_area.show(crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0996b-a009-462a-8c8e-0b511dfde9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natural_pdf.analyzers import Guides\n",
    "\n",
    "guides = Guides(table_area)\n",
    "\n",
    "# Add guides between the headers\n",
    "guides.vertical.from_content(\n",
    "    ['Statute', 'Description', 'Level', 'Repeat'],\n",
    "    align='between'\n",
    ")\n",
    "\n",
    "# Shift them around so they don't overlap the text\n",
    "guides.vertical.snap_to_whitespace(detection_method='text')\n",
    "\n",
    "# add in horizontal lines in places where 80% of the pixels are 'used'\n",
    "guides.horizontal.from_lines(threshold=0.8)\n",
    "\n",
    "# Honestly you could have done the same thing for the vertical lines\n",
    "# but it isn't as fun as .from_content, you know?\n",
    "# n=5 finds the 5 most likely places based on pixel density\n",
    "# guides.vertical.from_lines(n=5)\n",
    "\n",
    "guides.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cc7d7",
   "metadata": {},
   "source": [
    "You can just extract the data with `.extract_table()`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6471448-e3b6-471b-83e7-7e3d7c39df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = guides.extract_table().to_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21356f3a",
   "metadata": {},
   "source": [
    "But if you want to actually do things with specific columns or have more control, you can use `build_grid` to create regions for the table, columns, rows and cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "guides.build_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367d17b-0845-4c53-b933-387064b4451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.find_all('table_column')[-1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.find_all('table_row')[3].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1dd52e-a9b2-4499-981f-c446026e4cf3",
   "metadata": {},
   "source": [
    "### Figuring out information about things that are *not* text\n",
    "\n",
    "In a tiny preview of the next notebook: **what about those checkboxes?** Turns out we can use **image classification AI** to do it for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce24a7-12fd-41af-86b2-2b2c47360fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_col = table_area.find_all('table_column')[-1].expand(top=-40)\n",
    "last_col.show(crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cdd3e9-b5f4-4e52-a9c1-bf87de55855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = last_col.find_all(\"table_cell\")\n",
    "cells = cells.expand(left=-60, right=-175, top=-16, bottom=-16)\n",
    "cells.show(crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4950b9-9aa4-4d09-9f42-8ef8e549264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells.classify_all(['X', 'empty'], using='vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc274f-62f2-48b8-8816-395630cea471",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells.apply(lambda cell: (cell.category, cell.category_confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afede857",
   "metadata": {},
   "source": [
    "It's like magic! We'll look at it more in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fc19a-606e-4296-bc07-0f173de872b5",
   "metadata": {},
   "source": [
    "## Correcting OCR\n",
    "\n",
    "While we love OCR when it works, it often does *not* work great. We have a few solutions: send humans after it, or use LLMs or spell check to correct it.\n",
    "\n",
    "### With humans\n",
    "\n",
    "Let's ignore this for now. Some *stuff* out in the world got updated so it's kind of a wreck at the moment. I'm leaving it here so we can talk about it, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee0fd5-7285-4f00-b697-ce55e2d10dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from natural_pdf.utils.packaging import create_correction_task_package\n",
    "# from natural_pdf import PDF\n",
    "# import natural_pdf as npdf\n",
    "# npdf.options.image.width = 500\n",
    "\n",
    "# pdf = npdf.PDF(\"https://github.com/jsoma/abraji25-pdfs/raw/refs/heads/main/needs-ocr.pdf\")\n",
    "\n",
    "# page = pdf.pages[0]\n",
    "# page.apply_ocr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1c610-8af5-4354-99c2-59f6d70c42f9",
   "metadata": {},
   "source": [
    "After we apply OCR we can export to a magic format that we can display and fix up separately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f03719-4bdb-4b32-acd6-948e426e2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = page.extract_text()\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc927e-30d6-4646-b37f-4c5b7fd918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_correction_task_package(pdf, \"correction_package.zip\", overwrite=True, resolution=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdfdbc-1ac5-4f6f-b107-3659ff94bf06",
   "metadata": {},
   "source": [
    "### With LLMs\n",
    "\n",
    "Let's OCR at a low resolution, then see what our text looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db998c49-3c94-4332-974c-fb68ad2105a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.apply_ocr(resolution=50)\n",
    "page.find_all('text').inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7ac43-5869-4e25-bd67-a8f1333d7001",
   "metadata": {},
   "source": [
    "Some of these are pretty easy - for example, \"Uraanilary Warking Conditions\" should be \"Unsanity working conditions.\" OCR tools just don't know that kind of thing! But what if we could go through each piece of text, some some sort of spell check or something?\n",
    "\n",
    "You can use `correct_ocr` to change the text in a region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3ef62-1286-4a92-986c-9cdf6e96a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text_region(region):\n",
    "    return \"This is the updated text\"\n",
    "    \n",
    "page.correct_ocr(correct_text_region) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0ed4c-c888-4568-9435-7a1738544c75",
   "metadata": {},
   "source": [
    "And then, magically, all of our text is whatever we `return`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e06572-839b-4d30-99f5-9eeb843d7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.find_all('text').inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda455d-77ab-4335-b3a5-ea9e7a0cd9ee",
   "metadata": {},
   "source": [
    "But clearly we don't want the same thing every time! Let's add the bad OCR back in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64271e8-595a-41e3-9f63-19c58449a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-apply the OCR to break it again\n",
    "page.apply_ocr('surya', resolution=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa501818-7ff9-43e3-9ad9-218417360482",
   "metadata": {},
   "source": [
    "...and feed each line to an LLM trying to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817914da-0583-43f3-bbee-e701ac94a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from natural_pdf.ocr.utils import direct_ocr_llm\n",
    "\n",
    "client = OpenAI(api_key='API_KEY_GOES_HERE')\n",
    "\n",
    "prompt = \"\"\"\n",
    "Correct the spelling of this OCR'd text, a snippet of a document.\n",
    "Preserve original capitalization, punctuation, and symbols. \n",
    "Changing meaning is okay if it's clearly an OCR issue.\n",
    "Do not add any explanatory text, translations, comments, or quotation marks around the result.\n",
    "\"\"\"\n",
    "\n",
    "def correct_text_region(region):\n",
    "    text = region.extract_text()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-nano\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \"content\": prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    updated = completion.choices[0].message.content\n",
    "\n",
    "    if text != updated:    \n",
    "        print(f\"OLD: {text}\\nNEW:{updated}\") \n",
    "\n",
    "    return updated\n",
    "\n",
    "page.correct_ocr(correct_text_region) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34553785-0ce9-446f-b922-8ded673e24ae",
   "metadata": {},
   "source": [
    "And now we can use `.extract_text()` the magicaly same way.\n",
    "\n",
    "The real benefit of this vs sending the whole document to the LLM is *we don't change where the text is*. An LLM might OCR something for us, but it *loses the spatial context that we find so important*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0d23a-877e-422f-8636-b0960bb84463",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2e0c6-5a42-44fd-9827-924453ee354c",
   "metadata": {},
   "source": [
    "## Let's do the OCR with the LLM, period\n",
    "\n",
    "But if the LLM is *that good* at OCR, we can also find pieces of the page we would like to OCR and *send them each in isolation to the LLM*. We use `detect_only=True` so it doesn't try to figure out what the text is, just that the text is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98294bd-fbd4-4222-b47a-997830c37b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.apply_ocr('surya', detect_only=True)\n",
    "page.find_all('text').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f9638-2bc4-4b72-82e7-81ffb4ddf49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.find_all('text').inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca61c72-d112-457d-85a8-19bb22ff7ad5",
   "metadata": {},
   "source": [
    "Now we'll do an even fancier `correct_text_region`: it takes the region as an image, and sends it right on over to the LLM for OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca814b89-bd2f-4a4e-8da4-7d2b5d9af006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from natural_pdf.ocr.utils import direct_ocr_llm\n",
    "\n",
    "client = OpenAI(api_key='API_KEY_GOES_HERE')\n",
    "\n",
    "prompt = \"\"\"OCR this image patch. Return only the exact text content visible in the image. \n",
    "Preserve original spelling, capitalization, punctuation, and symbols.\n",
    "Fix misspellings if they are the result of blurry or incorrect OCR.\n",
    "Do not add any explanatory text, translations, comments, or quotation marks around the result.\n",
    "If you cannot process the image or do not see any text, return an empty space.\n",
    "The text is from an inspection report of a slaughterhouse.\"\"\"\n",
    "# The text is likely from a Greek document, potentially a spreadsheet, containing Modern Greek words or numbers\n",
    "\n",
    "def correct_text_region(region):\n",
    "    # Use a high resolution for the LLM call for best accuracy\n",
    "    return direct_ocr_llm(\n",
    "        region, \n",
    "        client, \n",
    "        prompt=prompt, \n",
    "        resolution=150, \n",
    "        model=\"gpt-4o\" \n",
    "    )\n",
    "\n",
    "page.correct_ocr(correct_text_region) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d108124-f9bf-4148-a902-5772a149b1ec",
   "metadata": {},
   "source": [
    "What do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ad115-f770-4d37-b57b-b6bf78ed2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.find_all('text').inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390d453-90f1-421b-b92a-2c14df1502ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = page.extract_text()\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (natural-pdf project venv)",
   "language": "python",
   "name": "natural-pdf-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
